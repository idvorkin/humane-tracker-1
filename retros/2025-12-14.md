# Retro: 2025-12-14

## Summary Metrics

- **Period:** Dec 7-14, 2025
- **Messages:** ~37,000+ (huge week)
- **Sessions:** ~122 sessions
- **Commits:** 67 commits merged
- **Projects analyzed:** humane-tracker-1

## What Went Well

- **Tag system fully implemented:** Complete tag habit system with nested tags, completion semantics, direct tag entries
- **Affirmations feature:** Added affirmation logs with DB persistence, import/export
- **Dexie Cloud improvements:** Added sync diagnostics, stale auth detection, sign-in dialog
- **Testing improvements:** Added component tests for TagChildPicker, E2E tests for tag completion
- **Variants removed:** Successfully refactored from variants to tag-based system
- **Pre-commit hooks strengthened:** Added `tsc --noEmit` to catch type errors
- **Git workflow documented:** Added feature branch workflow and rebase guidance

## What Didn't Go Well

- **Git workflow confusion:** Repeated confusion between origin vs upstream, PRs to wrong remote
- **Test strategy mismatches:** Created E2E tests when component tests were more appropriate
- **Date handling bugs:** Timezone issues in tests despite extensive documentation
- **Sync debugging time sink:** Hours spent on Dexie Cloud sync issues
- **Requirement understanding:** Made assumptions without running dev server to verify

## Friction Analysis

| Pattern | Example | What Claude Did Wrong | Improvement |
|---------|---------|----------------------|-------------|
| Origin vs Upstream confusion | "Why are all the PRs fucked up?" "No the pull should be against upstream" | Created PRs to origin instead of upstream, lost track of branch state | Add explicit pre-PR checklist in CLAUDE.md |
| Wrong test type | "Is E2E test the right way? Shouldn't this be component test?" | Defaulted to E2E when component tests more appropriate | Add test strategy decision tree |
| No empirical verification | "No no no. Run the server, load default habits, look at mobility" | Tried to infer UI behavior from code instead of running dev server | Add "always verify empirically" rule |
| Date handling in tests | Test failed: "expected '2025-01-14' to be '2025-01-15'" | Used raw Date() in tests causing timezone flakiness | Add test-specific date handling section |
| Tests don't catch bugs | "Why did our tag specification catch none of this?" | Feature code without comprehensive edge case coverage | Strengthen TDD requirement |
| Over-engineering | Multiple corrections to simplify solutions | Proposed complex solutions when simpler ones existed | Ask "what's the simplest solution?" first |

## Workflow Recommendations

| Pattern | Recommendation | Status |
|---------|---------------|--------|
| Git remote confusion | Add pre-PR checklist: check branch, fetch upstream, rebase, verify target | applied |
| Test type selection | Add decision tree: user behavior→E2E, component logic→component, pure functions→unit | applied |
| UI behavior questions | Rule: ALWAYS run dev server before answering UI questions | applied |
| Date handling in tests | Require toDateString() helper in all test date comparisons | applied |
| Dexie Cloud debugging | Document: search web first, known issues like stale auth tokens | applied |
| Feature completion | Add checklist: tests first, edge cases, dev server verification | applied |

## Action Items

- [x] Update CLAUDE.md with git workflow decision tree/checklist
- [x] Add test strategy decision framework to CLAUDE.md
- [x] Add "empirical verification rule" to development workflow
- [x] Strengthen date handling section with test-specific patterns
- [x] Add Dexie Cloud troubleshooting section
- [x] Add feature completion checklist

## Required Doc Reviews

- [x] **ARCHITECTURE.md** - N/A (no separate architecture doc)
- [x] **TEST_STRATEGY.md** - Added to CLAUDE.md "Test Strategy: When to Use Each Test Type"
- [x] **CLAUDE.md** - All 6 updates applied

## Detailed Friction Incidents

### 1. Git Workflow (7 incidents)

**Most impactful.** Multiple sessions had confusion about:
- Which remote to target for PRs (origin vs upstream)
- Whether to rebase before PR creation
- Which branch agent was on
- Tracking PR state across sessions

**Evidence:**
- "Is this main or upstream?" / "It needs to go against origin" / "No the pull should be against upstream"
- "Why are all the PRs fucked up? Is there something wrong with Upstream and Origin?"
- "How can they, we need to rebase? Why do we keep messing that up?"

### 2. Test Strategy (5 incidents)

**Pattern:** Claude defaults to E2E tests when component tests are more appropriate.

**Evidence:**
- "Is end-to-end test the right way to fix this? How does the component consume it?"
- "Can we test this with component tests?" (answer: yes, should have from start)

### 3. Requirement Understanding (6+ incidents)

**Pattern:** Claude makes assumptions from code reading instead of verifying in running app.

**Evidence:**
- User had to say "Run a server" → "Look at mobility" → "No no no, run the server, load default habits, look at mobility" (4 corrections before verification)

### 4. Date Handling (recurring)

Despite extensive CLAUDE.md documentation on date handling, tests still created with timezone-sensitive logic.

### 5. Sync Debugging

Dexie Cloud sync issues consumed significant time. Root cause was often stale auth tokens - a known issue that should be checked first, not debugged in code.

## Meta-Analysis

**Why these patterns persist:**

1. **Documentation vs Practice Gap:** CLAUDE.md has good info but not consistently applied under pressure
2. **Assumption Bias:** Defaults to inferring from code vs empirical verification
3. **Context Loss:** Loses track of state (PRs, branches, what's implemented) across multi-step tasks
4. **Complexity Preference:** Gravitates toward complex solutions, needs pushback to simplify
5. **External Dependencies:** Treats third-party service issues as code bugs instead of searching for known issues

## Next Steps

1. ~~Apply the 6 CLAUDE.md updates recommended in Workflow Recommendations~~ DONE
2. Consider adding automated checks for common mistakes
3. Review this retro with the team before next week's sprint
